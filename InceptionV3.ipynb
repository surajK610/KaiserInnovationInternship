{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "base_model= InceptionV3(include_top=False, weights=\"imagenet\", input_shape=(299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model= Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.40))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(7, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"skin-cancer-mnist-ham10000/HAM10000_metadata.csv\")\n",
    "data['image_full_name']=data['image_id']+'.jpg'\n",
    "X=data[['image_full_name','dx','lesion_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y=X.pop('dx').to_frame()\n",
    "X_train, X_test, y_train, y_test   = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "X_train,X_val,y_train,y_val        =train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat([X_train,y_train],axis=1)\n",
    "val=pd.concat([X_val,y_val],axis=1)\n",
    "test=pd.concat([X_test,y_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder= LabelEncoder()\n",
    "encoder.fit(val['dx'])\n",
    "name_as_indexes_train=encoder.transform(val['dx']) \n",
    "val['label']=name_as_indexes_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder()\n",
    "encoder.fit(test['dx'])\n",
    "name_as_indexes_test=encoder.transform(test['dx']) \n",
    "test['label']=name_as_indexes_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(rescale = 1./255,\n",
    "                                     rotation_range=10,  \n",
    "                                     zoom_range = 0.1, \n",
    "                                     width_shift_range=0.0,  height_shift_range=0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6009 validated image filenames belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data= train_generator.flow_from_dataframe(dataframe=train,x_col=\"image_full_name\",y_col=\"dx\",\n",
    "                                                batch_size=64,directory=\"HAM1000_images/HAM1000_images\",\n",
    "                                                shuffle=True,class_mode=\"categorical\",target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_generator=ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2003 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_data= test_generator.flow_from_dataframe(dataframe=test,x_col=\"image_full_name\",y_col=\"dx\",\n",
    "                                              directory=\"HAM1000_images/HAM1000_images\",\n",
    "                                              shuffle=False,batch_size=1,class_mode=None,target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2003 validated image filenames belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data=test_generator.flow_from_dataframe(dataframe=val,x_col=\"image_full_name\",y_col=\"dx\",\n",
    "                                            directory=\"HAM1000_images/HAM1000_images\",\n",
    "                                            batch_size=64,shuffle=False,class_mode=\"categorical\",target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "learning_control = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# Save the model with best weights\n",
    "checkpointer = ModelCheckpoint('best.hdf5', verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 137s 1s/step - loss: 0.3061 - acc: 0.9020 - val_loss: 1.0280 - val_acc: 0.7707\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.02800, saving model to best.hdf5\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 117s 1s/step - loss: 0.1116 - acc: 0.9691 - val_loss: 1.3092 - val_acc: 0.7540\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.02800\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0939 - acc: 0.9758 - val_loss: 0.9765 - val_acc: 0.8071\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.02800 to 0.97649, saving model to best.hdf5\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0852 - acc: 0.9766 - val_loss: 0.9508 - val_acc: 0.8319\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.97649 to 0.95081, saving model to best.hdf5\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0732 - acc: 0.9778 - val_loss: 0.9734 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.95081\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0697 - acc: 0.9793 - val_loss: 1.4598 - val_acc: 0.7777\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.95081\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0500 - acc: 0.9845 - val_loss: 1.2191 - val_acc: 0.7989\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.95081\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0600 - acc: 0.9835 - val_loss: 1.0489 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.95081\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 118s 1s/step - loss: 0.0440 - acc: 0.9884 - val_loss: 1.0126 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.95081\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0187 - acc: 0.9951 - val_loss: 1.0784 - val_acc: 0.8128\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.95081\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0155 - acc: 0.9958 - val_loss: 1.2589 - val_acc: 0.8009\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.95081\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0109 - acc: 0.9970 - val_loss: 0.9648 - val_acc: 0.8355\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.95081\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 121s 1s/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.9689 - val_acc: 0.8370\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.95081\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0062 - acc: 0.9987 - val_loss: 1.0028 - val_acc: 0.8334\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.95081\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0035 - acc: 0.9995 - val_loss: 0.9399 - val_acc: 0.8453\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.95081 to 0.93990, saving model to best.hdf5\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0062 - acc: 0.9985 - val_loss: 0.9566 - val_acc: 0.8432\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.93990\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.9839 - val_acc: 0.8417\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.93990\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0037 - acc: 0.9997 - val_loss: 0.9951 - val_acc: 0.8442\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.93990\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0056 - acc: 0.9988 - val_loss: 1.0471 - val_acc: 0.8350\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.93990\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.9957 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.93990\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0016 - acc: 0.9995 - val_loss: 1.0129 - val_acc: 0.8396\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.93990\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0011 - acc: 0.9998 - val_loss: 1.0091 - val_acc: 0.8442\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.93990\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0014 - acc: 0.9998 - val_loss: 1.0131 - val_acc: 0.8473\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.93990\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.9009 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.93990 to 0.90085, saving model to best.hdf5\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0017 - acc: 0.9993 - val_loss: 0.9450 - val_acc: 0.8551\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.90085\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0041 - acc: 0.9990 - val_loss: 1.0797 - val_acc: 0.8406\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.90085\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0050 - acc: 0.9990 - val_loss: 0.9504 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.90085\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0012 - acc: 0.9998 - val_loss: 1.0175 - val_acc: 0.8427\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.90085\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0032 - acc: 0.9992 - val_loss: 1.0460 - val_acc: 0.8412\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.90085\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 122s 1s/step - loss: 0.0024 - acc: 0.9993 - val_loss: 1.0260 - val_acc: 0.8360\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.90085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9e4716f60>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.adam(lr=0.0001),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit_generator(generator=train_data,\n",
    "                            steps_per_epoch=train_data.samples//train_data.batch_size,\n",
    "                            validation_data=val_data,\n",
    "                            verbose=1,\n",
    "                            validation_steps=val_data.samples//val_data.batch_size,\n",
    "                            epochs=30,callbacks=[learning_control, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('last.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003/2003 [==============================] - 36s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "test_data.reset()\n",
    "predictions = model.predict_generator(test_data, steps=test_data.samples/test_data.batch_size,verbose=1)\n",
    "y_pred= np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  26    4   28    0   11    0    0]\n",
      " [   2   59   21    0    7    4    0]\n",
      " [   3    2  182    1   24   16    0]\n",
      " [   2    2    7   13    3    1    0]\n",
      " [   2    0   31    0  170   22    1]\n",
      " [   0    2   69    0   78 1186    3]\n",
      " [   0    0    0    0    0    1   20]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "cm= confusion_matrix(name_as_indexes_test,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
