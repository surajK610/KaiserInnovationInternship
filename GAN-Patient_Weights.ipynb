{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN modified from perfect-match github repository \n",
    "\n",
    "Works for one variable (whole GAN-ITE structure).\n",
    "\n",
    "Not yet implemented in multivariable output scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/suraj/perfect_match-master/perfect_match-master\n",
      "Collecting Keras>=1.2.2 (from perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
      "Collecting tensorflow==1.4.0 (from perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/99/72/a420e22dc93416d30981e87a2318823ec09a9b18631369df0e7d9d164073/tensorflow-1.4.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting matplotlib>=1.3.1 (from perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/32/6b/0368cfa5e1d1ae169ab7dc78addda3fd5e6262e48d7373a9114bac7caff7/matplotlib-2.2.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting pandas>=0.18.0 (from perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/db/83/7d4008ffc2988066ff37f6a0bb6d7b60822367dcb36ba5e39aa7801fda54/pandas-0.24.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting h5py>=2.6.0 (from perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/53/08/27e4e9a369321862ffdce80ff1770553e9daec65d98befb2e14e7478b698/h5py-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting scikit-learn==0.19.0 (from perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/0e/46/cf53c3d9fd71a4e85714aa56bd32ca607ad945b4284d1e76bc909c94295a/scikit_learn-0.19.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.14.1 (from perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/1f/c7/198496417c9c2f6226616cff7dedf2115a4f4d0276613bab842ec8ac1e23/numpy-1.16.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting rpy2==2.8.6 (from perfect-match==1.0.0)\n",
      "Collecting keras-applications>=1.0.6 (from Keras>=1.2.2->perfect-match==1.0.0)\n",
      "Collecting keras-preprocessing>=1.0.5 (from Keras>=1.2.2->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Collecting scipy>=0.14 (from Keras>=1.2.2->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/1d/f6/7c16d60aeb3694e5611976cb4f1eaf1c6b7f1e7c55771d691013405a02ea/scipy-1.2.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting pyyaml (from Keras>=1.2.2->perfect-match==1.0.0)\n",
      "Collecting six>=1.9.0 (from Keras>=1.2.2->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting tensorflow-tensorboard<0.5.0,>=0.4.0rc1 (from tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/64/cd/f3d14d441eb1c5228aaf7e12e8e94895ae73e9af50383e481610b34357bd/tensorflow_tensorboard-0.4.0-py2-none-any.whl\n",
      "Collecting mock>=2.0.0 (from tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting enum34>=1.1.6 (from tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
      "Collecting protobuf>=3.3.0 (from tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/60/19c2c3b563c8a5ebbc5f17982fd794f415cfc4633a8248ab3e23a47662bc/protobuf-3.9.1-cp27-cp27mu-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 1.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel (from tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/83/b4a77d044e78ad1a45610eb88f745be2fd2c6d658f9798a15e384b7d57c9/wheel-0.33.6-py2.py3-none-any.whl\n",
      "Collecting backports.weakref>=1.0rc1 (from tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting cycler>=0.10 (from matplotlib>=1.3.1->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting backports.functools-lru-cache (from matplotlib>=1.3.1->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/8e/2424c0e65c4a066e28f539364deee49b6451f8fcd4f718fefa50cc3dcf48/backports.functools_lru_cache-1.5-py2.py3-none-any.whl\n",
      "Collecting subprocess32 (from matplotlib>=1.3.1->perfect-match==1.0.0)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib>=1.3.1->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/3d/78/cb9248b2289ec31e301137cedbe4ca503a74ca87f88cdbfd2f8be52323bf/kiwisolver-1.1.0-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting pytz (from matplotlib>=1.3.1->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/87/76/46d697698a143e05f77bec5a526bf4e56a0be61d63425b68f4ba553b51f2/pytz-2019.2-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.1 (from matplotlib>=1.3.1->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib>=1.3.1->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/11/fa/0160cd525c62d7abd076a070ff02b2b94de589f1a9789774f17d7c54058e/pyparsing-2.4.2-py2.py3-none-any.whl\n",
      "Collecting singledispatch (from rpy2==2.8.6->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
      "Collecting bleach==1.5.0 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting futures>=3.1.1; python_version < \"3.2\" (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/d8/a6/f46ae3f1da0cd4361c344888f59ec2f5785e69c872e175a748ef6071cdb5/futures-3.3.0-py2-none-any.whl\n",
      "Collecting html5lib==0.9999999 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "Collecting werkzeug>=0.11.10 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/ab/d3bed6b92042622d24decc7aadc8877badf18aeca1571045840ad4956d3f/Werkzeug-0.15.5-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorflow-tensorboard<0.5.0,>=0.4.0rc1->tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting setuptools (from protobuf>=3.3.0->tensorflow==1.4.0->perfect-match==1.0.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/75/b3/0a106dfaf7f48aef638da80b32608617cc8de4b24a22c8cd3759c32e5d30/setuptools-41.1.0-py2.py3-none-any.whl (576kB)\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: perfect-match\n",
      "  Running setup.py bdist_wheel for perfect-match ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/suraj/.cache/pip/wheels/ae/17/63/f962b5d75c3cae7cfa51deab2b063373532acbaacfc47235b9\n",
      "Successfully built perfect-match\n",
      "Installing collected packages: numpy, six, h5py, keras-applications, keras-preprocessing, scipy, pyyaml, Keras, html5lib, bleach, setuptools, protobuf, futures, wheel, werkzeug, markdown, tensorflow-tensorboard, funcsigs, mock, enum34, backports.weakref, tensorflow, cycler, backports.functools-lru-cache, subprocess32, kiwisolver, pytz, python-dateutil, pyparsing, matplotlib, pandas, scikit-learn, singledispatch, rpy2, perfect-match\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Keras-2.2.4 backports.functools-lru-cache-1.5 backports.weakref-1.0.post1 bleach-1.5.0 cycler-0.10.0 enum34-1.1.6 funcsigs-1.0.2 futures-3.3.0 h5py-2.9.0 html5lib-0.9999999 keras-applications-1.0.8 keras-preprocessing-1.1.0 kiwisolver-1.1.0 markdown-3.1.1 matplotlib-2.2.4 mock-3.0.5 numpy-1.16.4 pandas-0.24.2 perfect-match-1.0.0 protobuf-3.9.1 pyparsing-2.4.2 python-dateutil-2.8.0 pytz-2019.2 pyyaml-5.1.2 rpy2-2.8.6 scikit-learn-0.20.4 scipy-1.2.2 setuptools-41.1.0 singledispatch-3.4.0.3 six-1.12.0 subprocess32-3.5.4 tensorflow-1.4.0 tensorflow-tensorboard-0.4.0 werkzeug-0.15.5 wheel-0.33.6\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install ../perfect_match-master/perfect_match-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: https://github.com/clinicalml/cfrnet, MIT-License\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "SQRT_CONST = 1e-10\n",
    "\n",
    "\n",
    "def get_nonlinearity_by_name(name):\n",
    "    if name.lower() == 'elu':\n",
    "        return tf.nn.elu\n",
    "    else:\n",
    "        return tf.nn.relu\n",
    "\n",
    "\n",
    "def build_mlp(x, num_layers=1, num_units=16, dropout=0.0,\n",
    "              nonlinearity=tf.nn.elu, weight_initialisation_std=0.1):\n",
    "    input_dim = int(x.shape[-1])\n",
    "    h_in, weights_in, biases_in = [x], [], []\n",
    "    for i in range(0, num_layers):\n",
    "        if i == 0:\n",
    "            ''' If using variable selection, first layer is just rescaling'''\n",
    "            weights_in.append(tf.Variable(tf.random_normal([input_dim, num_units],\n",
    "                                                           stddev=weight_initialisation_std / np.sqrt(input_dim))))\n",
    "        else:\n",
    "            weights_in.append(tf.Variable(tf.random_normal([num_units, num_units],\n",
    "                                                           stddev=weight_initialisation_std / np.sqrt(num_units))))\n",
    "\n",
    "        biases_in.append(tf.Variable(tf.zeros([1, num_units])))\n",
    "        z = tf.matmul(h_in[i], weights_in[i]) + biases_in[i]\n",
    "\n",
    "        h_in.append(nonlinearity(z))\n",
    "        h_in[i + 1] = tf.nn.dropout(h_in[i + 1], 1.0 - dropout)\n",
    "\n",
    "    h_rep = h_in[len(h_in) - 1]\n",
    "    return h_rep, weights_in, biases_in\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#from ..cfr.util import get_nonlinearity_by_name, build_mlp\n",
    "\n",
    "\n",
    "class GANITEBuilder(object):\n",
    "    @staticmethod\n",
    "    def build(input_dim, output_dim, num_units=128, dropout=0.0, l2_weight=0.0, learning_rate=0.0001, num_layers=2,\n",
    "              num_treatments=2, with_bn=False, nonlinearity=\"elu\", initializer=tf.variance_scaling_initializer(),\n",
    "              alpha=1.0, beta=1.0):\n",
    "        x = tf.placeholder(\"float\", shape=[None, input_dim], name='x')\n",
    "        t = tf.placeholder(\"float\", shape=[None, 1], name='t')\n",
    "        y_f = tf.placeholder(\"float\", shape=[None, output_dim], name='y_f')\n",
    "        y_full = tf.placeholder(\"float\", shape=[None, num_treatments], name='y_full')\n",
    "                \n",
    "        y_pred_cf, propensity_scores, z_g = GANITEBuilder.build_counterfactual_block(input_dim, x, t, y_f,\n",
    "                                                                                     num_units, dropout, l2_weight,\n",
    "                                                                                     learning_rate, num_layers,\n",
    "                                                                                     num_treatments, with_bn,\n",
    "                                                                                     nonlinearity, initializer)\n",
    "\n",
    "        y_pred_ite, d_ite_pred, d_ite_true, z_i = GANITEBuilder.build_ite_block(input_dim, x, t, y_f, y_full,\n",
    "                                                                                num_units, dropout, l2_weight,\n",
    "                                                                                learning_rate, num_layers,\n",
    "                                                                                num_treatments, with_bn,\n",
    "                                                                                nonlinearity, initializer)\n",
    "\n",
    "        # Build losses and optimizers.\n",
    "        t_one_hot = tf.one_hot(tf.cast(t, \"int32\"), num_treatments)\n",
    "\n",
    "        propensity_loss_cf = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=propensity_scores,\n",
    "                                                                                    labels=t_one_hot))\n",
    "\n",
    "        batch_size = tf.shape(y_pred_cf)[0]\n",
    "        indices = tf.stack([tf.range(batch_size), tf.cast(t, \"int32\")[:, 0]], axis=-1)\n",
    "        y_f_pred = tf.gather_nd(y_pred_cf, indices)\n",
    "\n",
    "        y_f_i = y_f  # tf.Print(y_f, [y_f[:, 0]], message=\"y_f=\", summarize=8)\n",
    "        y_f_pred_i = y_f_pred  # tf.Print(y_f_pred, [y_f_pred], message=\"y_f_pred=\", summarize=8)\n",
    "\n",
    "        supervised_loss_cf = tf.sqrt(tf.reduce_mean(tf.squared_difference(y_f_i[:, 0], y_f_pred_i)))\n",
    "\n",
    "        cf_discriminator_loss = propensity_loss_cf\n",
    "        cf_generator_loss = -propensity_loss_cf + alpha * supervised_loss_cf\n",
    "\n",
    "        # D_ITE goal: 0 when True, 1 when Pred\n",
    "        ite_loss = tf.reduce_mean(tf.log(d_ite_true)) + tf.reduce_mean(tf.log(1 - d_ite_pred))\n",
    "\n",
    "        y_full_i = y_full  # tf.Print(y_full, [y_full], message=\"y_full=\", summarize=8)\n",
    "        y_pred_ite_i = y_pred_ite  # tf.Print(y_pred_ite, [y_pred_ite], message=\"y_pred_ite=\", summarize=8)\n",
    "        supervised_loss_ite = tf.sqrt(tf.reduce_mean(tf.squared_difference(y_full_i, y_pred_ite_i)))\n",
    "\n",
    "        ite_discriminator_loss = -ite_loss\n",
    "        ite_generator_loss = ite_loss + beta * supervised_loss_ite\n",
    "        return cf_generator_loss, cf_discriminator_loss, ite_generator_loss, ite_discriminator_loss, \\\n",
    "               x, t, y_f, y_full, y_pred_cf, y_pred_ite, z_g, z_i\n",
    "\n",
    "    @staticmethod\n",
    "    def build_tarnet(mlp_input, t, input_dim, num_layers, num_units, dropout, num_treatments, nonlinearity):\n",
    "        initializer = tf.variance_scaling_initializer()\n",
    "        x = build_mlp(mlp_input, num_layers, num_units, dropout, nonlinearity)\n",
    "\n",
    "        all_indices, outputs = [], []\n",
    "        for i in range(num_treatments):\n",
    "            indices = tf.reshape(tf.to_int32(tf.where(tf.equal(tf.reshape(t, (-1,)), i))), (-1,))\n",
    "            current_last_layer_h = tf.gather(x, indices)\n",
    "\n",
    "            last_layer, _, _ = build_mlp(current_last_layer_h, num_layers, num_units, dropout, nonlinearity)\n",
    "\n",
    "            output = tf.layers.dense(last_layer, units=num_treatments, use_bias=True,\n",
    "                                     bias_initializer=initializer)\n",
    "\n",
    "            all_indices.append(indices)\n",
    "            outputs.append(output)\n",
    "        return tf.concat(outputs, axis=-1), all_indices\n",
    "\n",
    "    @staticmethod\n",
    "    def build_counterfactual_block(input_dim, x, t, y_f, num_units=128, dropout=0.0, l2_weight=0.0,\n",
    "                                   learning_rate=0.0001, num_layers=2,\n",
    "                                   num_treatments=2, with_bn=False, nonlinearity=\"elu\",\n",
    "                                   initializer=tf.variance_scaling_initializer()):\n",
    "\n",
    "        y_pred, z_g = GANITEBuilder.build_counterfactual_generator(input_dim, x, t, y_f, num_units,\n",
    "                                                                   dropout, l2_weight, learning_rate,\n",
    "                                                                   num_layers, num_treatments, with_bn,\n",
    "                                                                   nonlinearity,\n",
    "                                                                   initializer)\n",
    "\n",
    "        propensity_scores = GANITEBuilder.build_counterfactual_discriminator(input_dim, x, t, y_pred, num_units,\n",
    "                                                                             dropout, l2_weight, learning_rate,\n",
    "                                                                             num_layers, num_treatments, with_bn,\n",
    "                                                                             nonlinearity,\n",
    "                                                                             initializer)\n",
    "        return y_pred, propensity_scores, z_g\n",
    "\n",
    "    @staticmethod\n",
    "    def build_counterfactual_generator(input_dim, x, t, y_f, num_units=128, dropout=0.0, l2_weight=0.0,\n",
    "                                       learning_rate=0.0001, num_layers=2,\n",
    "                                       num_treatments=2, with_bn=False, nonlinearity=\"elu\",\n",
    "                                       initializer=tf.variance_scaling_initializer()):\n",
    "        nonlinearity = get_nonlinearity_by_name(nonlinearity)\n",
    "        with tf.variable_scope(\"g_cf\",\n",
    "                               initializer=initializer):\n",
    "            z_g = tf.placeholder(\"float\", shape=[None, num_treatments-1], name='z_g')\n",
    "\n",
    "            mlp_input = tf.concat([x, y_f, t, z_g], axis=-1)\n",
    "            x, _, _ = build_mlp(mlp_input, num_layers, num_units, dropout, nonlinearity)\n",
    "            y = tf.layers.dense(x, units=num_treatments, use_bias=True,\n",
    "                                bias_initializer=initializer)\n",
    "            #why is it returning only the num_treatments\n",
    "            return y, z_g\n",
    "\n",
    "    @staticmethod\n",
    "    def build_counterfactual_discriminator(input_dim, x, t, y_pred, num_units=128, dropout=0.0, l2_weight=0.0,\n",
    "                                           learning_rate=0.0001, num_layers=2,\n",
    "                                           num_treatments=2, with_bn=False, nonlinearity=\"elu\",\n",
    "                                           initializer=tf.variance_scaling_initializer(),\n",
    "                                           reuse=False):\n",
    "        nonlinearity = get_nonlinearity_by_name(nonlinearity)\n",
    "        with tf.variable_scope(\"d_cf\",\n",
    "                               reuse=reuse,\n",
    "                               initializer=initializer):\n",
    "            mlp_input = tf.concat([x, y_pred], axis=-1)\n",
    "            x, _, _ = build_mlp(mlp_input, num_layers, num_units, dropout, nonlinearity)\n",
    "            propensity_scores = tf.layers.dense(x, units=num_treatments, use_bias=True,\n",
    "                                                bias_initializer=initializer)\n",
    "            return propensity_scores\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def build_ite_block(input_dim, x, t, y_f, y_full, num_units=128, dropout=0.0, l2_weight=0.0,\n",
    "                        learning_rate=0.0001, num_layers=2,\n",
    "                        num_treatments=2, with_bn=False, nonlinearity=\"elu\",\n",
    "                        initializer=tf.variance_scaling_initializer()):\n",
    "        y_pred_ite, z_i = GANITEBuilder.build_ite_generator(input_dim, x, t, y_f, num_units,\n",
    "                                                        dropout, l2_weight, learning_rate,\n",
    "                                                        num_layers, num_treatments, with_bn,\n",
    "                                                        nonlinearity, initializer)\n",
    "\n",
    "        d_ite_pred = GANITEBuilder.build_ite_discriminator(input_dim, x, t, y_pred_ite, num_units,\n",
    "                                                           dropout, l2_weight, learning_rate,\n",
    "                                                           num_layers, num_treatments, with_bn,\n",
    "                                                           nonlinearity, initializer, reuse=False)\n",
    "\n",
    "        d_ite_true = GANITEBuilder.build_ite_discriminator(input_dim, x, t, y_full, num_units,\n",
    "                                                           dropout, l2_weight, learning_rate,\n",
    "                                                           num_layers, num_treatments, with_bn,\n",
    "                                                           nonlinearity, initializer, reuse=True)\n",
    "\n",
    "        return y_pred_ite, d_ite_pred, d_ite_true, z_i\n",
    "\n",
    "    @staticmethod\n",
    "    def build_ite_generator(input_dim, x, t, y_f, num_units=128, dropout=0.0, l2_weight=0.0,\n",
    "                            learning_rate=0.0001, num_layers=2,\n",
    "                            num_treatments=2, with_bn=False, nonlinearity=\"elu\",\n",
    "                            initializer=tf.variance_scaling_initializer()):\n",
    "        nonlinearity = get_nonlinearity_by_name(nonlinearity)\n",
    "        with tf.variable_scope(\"g_ite\",\n",
    "                               initializer=initializer):\n",
    "            z_i = tf.placeholder(\"float\", shape=[None, num_treatments], name='z_i')\n",
    "            mlp_input = tf.concat([x, z_i], axis=-1)\n",
    "            x, _, _ = build_mlp(mlp_input, num_layers, num_units, dropout, nonlinearity)\n",
    "            y_pred = tf.layers.dense(x, units=num_treatments, use_bias=True,\n",
    "                                     bias_initializer=initializer)\n",
    "            return y_pred, z_i\n",
    "\n",
    "    @staticmethod\n",
    "    def build_ite_discriminator(input_dim, x, t, y_pred, num_units=128, dropout=0.0, l2_weight=0.0,\n",
    "                                learning_rate=0.0001, num_layers=2,\n",
    "                                num_treatments=2, with_bn=False, nonlinearity=\"elu\",\n",
    "                                initializer=tf.variance_scaling_initializer(),\n",
    "                                reuse=False):\n",
    "        nonlinearity = get_nonlinearity_by_name(nonlinearity)\n",
    "        with tf.variable_scope(\"d_ite\",\n",
    "                               reuse=reuse,\n",
    "                               initializer=initializer):\n",
    "            mlp_input = tf.concat([x, y_pred], axis=-1)\n",
    "            x, _, _ = build_mlp(mlp_input, num_layers, num_units, dropout, nonlinearity)\n",
    "            y = tf.layers.dense(x, units=1, use_bias=True,\n",
    "                                bias_initializer=initializer, activation=tf.nn.sigmoid)\n",
    "            return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from perfect_match.models.baselines.ganite_package.ganite_builder import GANITEBuilder\n",
    "\n",
    "\n",
    "class GANITEModel(object):\n",
    "    def __init__(self, input_dim, output_dim, num_units=128, dropout=0.0, l2_weight=0.0, learning_rate=0.0001, num_layers=2,\n",
    "                 num_treatments=2, with_bn=False, nonlinearity=\"elu\", initializer=tf.variance_scaling_initializer(),\n",
    "                 alpha=1.0, beta=1.0):\n",
    "\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config)\n",
    "        self.num_treatments = num_treatments\n",
    "\n",
    "        self.cf_generator_loss, self.cf_discriminator_loss, \\\n",
    "        self.ite_generator_loss, self.ite_discriminator_loss, \\\n",
    "        self.x, self.t, self.y_f, self.y_full, self.y_pred_cf, self.y_pred_ite, self.z_g, self.z_i = \\\n",
    "            GANITEBuilder.build(input_dim, output_dim,\n",
    "                                num_units=num_units,\n",
    "                                dropout=dropout,\n",
    "                                l2_weight=l2_weight,\n",
    "                                learning_rate=learning_rate,\n",
    "                                num_layers=num_layers,\n",
    "                                num_treatments=num_treatments,\n",
    "                                with_bn=with_bn,\n",
    "                                nonlinearity=nonlinearity,\n",
    "                                initializer=initializer,\n",
    "                                alpha=alpha,\n",
    "                                beta=beta)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_scoped_variables(scope_name):\n",
    "        t_vars = tf.trainable_variables()\n",
    "        vars = [var for var in t_vars if scope_name in var.name]\n",
    "        return vars\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cf_generator_vairables():\n",
    "        return GANITEModel.get_scoped_variables(\"g_cf\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cf_discriminator_vairables():\n",
    "        return GANITEModel.get_scoped_variables(\"d_cf\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_ite_generator_vairables():\n",
    "        return GANITEModel.get_scoped_variables(\"g_ite\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_ite_discriminator_vairables():\n",
    "        return GANITEModel.get_scoped_variables(\"d_ite\")\n",
    "\n",
    "    def load(self, path):\n",
    "        saver = tf.train.Saver()\n",
    "        # saver.restore(self.sess, path)\n",
    "\n",
    "    def train(self, train_generator, train_steps, val_generator, val_steps, num_epochs,\n",
    "              learning_rate, learning_rate_decay=0.97, iterations_per_decay=100,\n",
    "              dropout=0.0, imbalance_loss_weight=0.0, l2_weight=0.0, checkpoint_path=\"\",\n",
    "              early_stopping_patience=12, early_stopping_on_pehe=False):\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "        global_step_1 = tf.Variable(0, trainable=False, dtype=\"int64\")\n",
    "        global_step_2 = tf.Variable(0, trainable=False, dtype=\"int64\")\n",
    "        global_step_3 = tf.Variable(0, trainable=False, dtype=\"int64\")\n",
    "        global_step_4 = tf.Variable(0, trainable=False, dtype=\"int64\")\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        train_step_g_cf = opt.minimize(self.cf_generator_loss, global_step=global_step_1,\n",
    "                                       var_list=GANITEModel.get_cf_generator_vairables())\n",
    "        train_step_d_cf = opt.minimize(self.cf_discriminator_loss, global_step=global_step_2,\n",
    "                                       var_list=GANITEModel.get_cf_discriminator_vairables())\n",
    "        train_step_g_ite = opt.minimize(self.ite_generator_loss, global_step=global_step_3,\n",
    "                                        var_list=GANITEModel.get_ite_generator_vairables())\n",
    "        train_step_d_ite = opt.minimize(self.ite_discriminator_loss, global_step=global_step_4,\n",
    "                                        var_list=GANITEModel.get_ite_discriminator_vairables())\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        best_val_loss, num_epochs_without_improvement = np.finfo(float).max, 0\n",
    "        print(\"COUNTERFACTUAL TRAINING\")\n",
    "        for epoch_idx in range(num_epochs):\n",
    "            for step_idx in range(train_steps):\n",
    "                train_losses_g = self.run_generator(train_generator, 1, self.cf_generator_loss, train_step_g_cf)\n",
    "                train_losses_d = self.run_generator(train_generator, 1, self.cf_discriminator_loss, train_step_d_cf)\n",
    "\n",
    "            val_losses_g = self.run_generator(val_generator, val_steps, self.cf_generator_loss)\n",
    "            val_losses_d = self.run_generator(val_generator, val_steps, self.cf_discriminator_loss)\n",
    "\n",
    "            current_val_loss = val_losses_g[0]\n",
    "            do_save = current_val_loss < best_val_loss\n",
    "            if do_save:\n",
    "                num_epochs_without_improvement = 0\n",
    "                best_val_loss = current_val_loss\n",
    "                saver.save(self.sess, checkpoint_path)\n",
    "            else:\n",
    "                num_epochs_without_improvement += 1\n",
    "\n",
    "            self.print_losses(epoch_idx, num_epochs,\n",
    "                              [train_losses_g[0], train_losses_d[0]],\n",
    "                              [val_losses_g[0], val_losses_d[0]],\n",
    "                              do_save)\n",
    "\n",
    "            if num_epochs_without_improvement >= early_stopping_patience:\n",
    "                break\n",
    "\n",
    "        best_val_loss, num_epochs_without_improvement = np.finfo(float).max, 0\n",
    "        \n",
    "        print(\"ITE TRAINING\")\n",
    "        for epoch_idx in range(num_epochs):\n",
    "            for step_idx in range(train_steps):\n",
    "                train_losses_g = self.run_generator(train_generator, 1, self.ite_generator_loss, train_step_g_ite,\n",
    "                                                    include_y_full=True)\n",
    "                train_losses_d = self.run_generator(train_generator, 1, self.ite_discriminator_loss, train_step_d_ite,\n",
    "                                                    include_y_full=True)\n",
    "            val_losses_g = self.run_generator(val_generator, val_steps, self.ite_generator_loss,\n",
    "                                              include_y_full=True)\n",
    "            val_losses_d = self.run_generator(val_generator, val_steps, self.ite_discriminator_loss,\n",
    "                                              include_y_full=True)\n",
    "\n",
    "            current_val_loss = val_losses_g[0]\n",
    "            do_save = current_val_loss < best_val_loss\n",
    "            if do_save:\n",
    "                num_epochs_without_improvement = 0\n",
    "                best_val_loss = current_val_loss\n",
    "                saver.save(self.sess, checkpoint_path)\n",
    "            else:\n",
    "                num_epochs_without_improvement += 1\n",
    "\n",
    "            self.print_losses(epoch_idx, num_epochs,\n",
    "                              [train_losses_g[0], train_losses_d[0]],\n",
    "                              [val_losses_g[0], val_losses_d[0]],\n",
    "                              do_save)\n",
    "\n",
    "            if num_epochs_without_improvement >= early_stopping_patience:\n",
    "                break\n",
    "\n",
    "    def print_losses(self, epoch_idx, num_epochs, train_losses, val_losses, did_save=False):\n",
    "        print(\"Epoch [{:04d}/{:04d}] {:} TRAIN: G={:.3f} D={:.3f} VAL: G={:.3f} D={:.3f}\"\n",
    "              .format(\n",
    "                  epoch_idx, num_epochs,\n",
    "                  \"xx\" if did_save else \"::\",\n",
    "                  train_losses[0], train_losses[1],\n",
    "                  val_losses[0], val_losses[1]\n",
    "              ),\n",
    "              file=sys.stderr)\n",
    "\n",
    "    def run_generator(self, generator, steps, loss, train_step=None, include_y_full=False):\n",
    "        losses = []\n",
    "        for iter_idx in range(steps):\n",
    "            (x_batch, t_batch), y_batch = generator()\n",
    "            t_batch = np.expand_dims(t_batch, axis=-1)\n",
    "            y_batch = np.expand_dims(y_batch, axis=-1)\n",
    "\n",
    "            batch_size = len(x_batch)\n",
    "            feed_dict = {\n",
    "                self.x: x_batch,\n",
    "                self.t: t_batch,\n",
    "                self.y_f: y_batch,\n",
    "                self.z_g: np.random.uniform(size=(batch_size, self.num_treatments-1)),\n",
    "                self.z_i: np.random.uniform(size=(batch_size, self.num_treatments))\n",
    "            }\n",
    "            if include_y_full:\n",
    "                y_pred = self._predict_g_cf([x_batch, t_batch], y_batch)\n",
    "                \n",
    "                y_pred[np.arange(len(y_pred)), t_batch] = y_batch\n",
    "                feed_dict[self.y_full] = y_pred\n",
    "\n",
    "            if train_step is not None:\n",
    "                self.sess.run(train_step, feed_dict=feed_dict)\n",
    "\n",
    "            losses.append(self.sess.run([loss],\n",
    "                                        feed_dict=feed_dict))\n",
    "        return np.mean(losses, axis=0)\n",
    "\n",
    "    def _predict_g_cf(self, x, y_f):\n",
    "        batch_size = len(x[0])\n",
    "        y_pred = self.sess.run(self.y_pred_cf, feed_dict={\n",
    "            self.x: x[0],\n",
    "            self.t: x[1],\n",
    "            self.y_f: y_f,\n",
    "            self.z_g: np.random.uniform(size=(batch_size, self.num_treatments-1))\n",
    "        })\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, x):\n",
    "        batch_size = len(x[0])\n",
    "        y_pred = self.sess.run(self.y_pred_ite, feed_dict={\n",
    "             self.x: x[0],\n",
    "             self.z_i: np.random.uniform(size=(batch_size, self.num_treatments))\n",
    "        })\n",
    "        y_pred = np.array(map(lambda inner, idx: inner[idx], y_pred, x[1]))\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "#from perfect_match.models.model_factory import ModelFactory\n",
    "\n",
    "\n",
    "class Baseline(object):\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    @staticmethod\n",
    "    def to_data_frame(x):\n",
    "        return pd.DataFrame(data=x, index=np.arange(x.shape[0]), columns=np.arange(x.shape[1]))\n",
    "\n",
    "    def _build(self, **kwargs):\n",
    "        return None\n",
    "\n",
    "    def build(self, **kwargs):\n",
    "        self.model = self._build(**kwargs)\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        return x\n",
    "\n",
    "    def postprocess(self, y):\n",
    "        return y\n",
    "\n",
    "    def load(self, path):\n",
    "        pass\n",
    "\n",
    "    def save(self, path):\n",
    "        pass\n",
    "\n",
    "    def predict_for_model(self, model, x):\n",
    "        if hasattr(self.model, \"predict_proba\"):\n",
    "            return self.postprocess(model.predict_proba(self.preprocess(x)))\n",
    "        else:\n",
    "            return self.postprocess(model.predict(self.preprocess(x)))\n",
    "\n",
    "    def predict(self, x):\n",
    "        a = x[0]\n",
    "        b = x[1]\n",
    "        #return self.model._predict_g_cf(a, b)\n",
    "        return self.predict_for_model(self.model, x)\n",
    "\n",
    "    def fit_generator_for_model(self, model, train_generator, train_steps, val_generator, val_steps, num_epochs):\n",
    "        x, y = self.collect_generator(train_generator, train_steps)\n",
    "        model.fit(x, y)\n",
    "\n",
    "    def fit_generator(self, train_generator, train_steps, val_generator, val_steps, num_epochs, batch_size):\n",
    "        self.fit_generator_for_model(self.model, train_generator, train_steps, val_generator, val_steps, num_epochs)\n",
    "\n",
    "    def collect_generator(self, generator, generator_steps):\n",
    "        all_outputs = []\n",
    "        for _ in range(generator_steps):\n",
    "            generator_output = next(generator)\n",
    "            x, y = generator_output[0], generator_output[1]\n",
    "            all_outputs.append((self.preprocess(x), y))\n",
    "        return map(partial(np.concatenate, axis=0), zip(*all_outputs))\n",
    "\n",
    "\n",
    "class PickleableMixin(object):\n",
    "    def load(self, path):\n",
    "        self.model = ModelFactory.load_object(path)\n",
    "\n",
    "    def save(self, path):\n",
    "        ModelFactory.save_object(self.model, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "#from perfect_match.models.baselines.baseline import Baseline\n",
    "#from perfect_match.models.baselines.ganite_package.ganite_model import GANITEModel\n",
    "\n",
    "\n",
    "class GANITE(Baseline):\n",
    "    def __init__(self):\n",
    "        super(GANITE, self).__init__()\n",
    "        self.callbacks = []\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model.load(path)\n",
    "\n",
    "    def _build(self, **kwargs):\n",
    "        self.best_model_path = kwargs[\"best_model_path\"]\n",
    "        self.learning_rate = kwargs[\"learning_rate\"]\n",
    "        self.dropout = kwargs[\"dropout\"]\n",
    "        self.l2_weight = kwargs[\"l2_weight\"]\n",
    "        self.num_units = kwargs[\"num_units\"]\n",
    "        self.num_layers = kwargs[\"num_layers\"]\n",
    "        self.num_treatments = kwargs[\"num_treatments\"]\n",
    "        self.imbalance_loss_weight = kwargs[\"imbalance_loss_weight\"]\n",
    "        self.early_stopping_patience = kwargs[\"early_stopping_patience\"]\n",
    "        self.early_stopping_on_pehe = kwargs[\"early_stopping_on_pehe\"]\n",
    "        self.input_dim = kwargs[\"input_dim\"]\n",
    "        self.output_dim = kwargs[\"output_dim\"]\n",
    "        self.ganite_weight_alpha = kwargs[\"ganite_weight_alpha\"]\n",
    "        self.ganite_weight_beta = kwargs[\"ganite_weight_beta\"]\n",
    "        return GANITEModel(self.input_dim,\n",
    "                           self.output_dim,\n",
    "                           num_units=self.num_units,\n",
    "                           dropout=self.dropout,\n",
    "                           l2_weight=self.l2_weight,\n",
    "                           learning_rate=self.learning_rate,\n",
    "                           num_layers=self.num_layers,\n",
    "                           num_treatments=self.num_treatments,\n",
    "                           with_bn=False,\n",
    "                           nonlinearity=\"elu\",\n",
    "                           alpha=self.ganite_weight_alpha,\n",
    "                           beta=self.ganite_weight_beta)\n",
    "\n",
    "    def fit_generator(self, train_generator, train_steps, val_generator, val_steps, num_epochs, batch_size):\n",
    "        # num_epochs = int(np.ceil(3000 / batch_size))\n",
    "        self.model.train(train_generator,\n",
    "                         train_steps,\n",
    "                         num_epochs=num_epochs,\n",
    "                         learning_rate=self.learning_rate,\n",
    "                         val_generator=val_generator,\n",
    "                         val_steps=val_steps,\n",
    "                         dropout=self.dropout,\n",
    "                         l2_weight=self.l2_weight,\n",
    "                         imbalance_loss_weight=self.imbalance_loss_weight,\n",
    "                         checkpoint_path=self.best_model_path,\n",
    "                         early_stopping_patience=self.early_stopping_patience,\n",
    "                         early_stopping_on_pehe=self.early_stopping_on_pehe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GANITE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict()\n",
    "kwargs[\"best_model_path\"] = \".\"\n",
    "kwargs[\"learning_rate\"] = 0.0001\n",
    "kwargs[\"dropout\"] = 0.05\n",
    "kwargs[\"l2_weight\"] = 0.001 \n",
    "kwargs[\"num_units\"] = 90\n",
    "kwargs[\"num_layers\"] = 3\n",
    "kwargs[\"num_treatments\"] = 2\n",
    "kwargs[\"imbalance_loss_weight\"] = 0.0\n",
    "kwargs[\"early_stopping_patience\"] = 70 \n",
    "kwargs[\"early_stopping_on_pehe\"] = 0\n",
    "kwargs[\"input_dim\"] = 90\n",
    "kwargs[\"output_dim\"] = 1 \n",
    "kwargs[\"ganite_weight_alpha\"] = 1\n",
    "kwargs[\"ganite_weight_beta\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_model_path': '.',\n",
       " 'dropout': 0.05,\n",
       " 'early_stopping_on_pehe': 0,\n",
       " 'early_stopping_patience': 70,\n",
       " 'ganite_weight_alpha': 1,\n",
       " 'ganite_weight_beta': 1,\n",
       " 'imbalance_loss_weight': 0.0,\n",
       " 'input_dim': 90,\n",
       " 'l2_weight': 0.001,\n",
       " 'learning_rate': 0.0001,\n",
       " 'num_layers': 3,\n",
       " 'num_treatments': 2,\n",
       " 'num_units': 90,\n",
       " 'output_dim': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('../autoencoder/DIABETES_DATA_100k.tsv', delimiter='\\t').drop(columns = ['DX_EMB', 'Unnamed: 0'])\n",
    "\n",
    "for a in diabetes.columns:\n",
    "    diabetes[a].fillna(diabetes[a].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diabetes = diabetes.drop(['P_ID', 'COMMENT 01', 'RACE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = diabetes.drop(['P_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>FEMALE</th>\n",
       "      <th>BP_SYSTOLIC</th>\n",
       "      <th>BP_DIASTOLIC</th>\n",
       "      <th>TEMPERATURE</th>\n",
       "      <th>PULSE</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>BMI</th>\n",
       "      <th>ALT</th>\n",
       "      <th>...</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>DIABETES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>73.50</td>\n",
       "      <td>97.160</td>\n",
       "      <td>65.000</td>\n",
       "      <td>2576.1360</td>\n",
       "      <td>67.514625</td>\n",
       "      <td>24.939445</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356847</td>\n",
       "      <td>1.847694</td>\n",
       "      <td>0.361279</td>\n",
       "      <td>-4.518900</td>\n",
       "      <td>0.127954</td>\n",
       "      <td>0.369101</td>\n",
       "      <td>-0.244062</td>\n",
       "      <td>-0.386845</td>\n",
       "      <td>-0.151202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121.461538</td>\n",
       "      <td>68.00</td>\n",
       "      <td>98.275</td>\n",
       "      <td>83.000</td>\n",
       "      <td>2570.8500</td>\n",
       "      <td>75.545455</td>\n",
       "      <td>19.955741</td>\n",
       "      <td>22.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015880</td>\n",
       "      <td>3.052436</td>\n",
       "      <td>-0.607785</td>\n",
       "      <td>-10.509056</td>\n",
       "      <td>0.301617</td>\n",
       "      <td>1.731270</td>\n",
       "      <td>-7.489346</td>\n",
       "      <td>-0.624842</td>\n",
       "      <td>-0.290411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.750000</td>\n",
       "      <td>60.25</td>\n",
       "      <td>98.175</td>\n",
       "      <td>52.375</td>\n",
       "      <td>3275.5925</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>27.762605</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021642</td>\n",
       "      <td>1.009353</td>\n",
       "      <td>-1.400726</td>\n",
       "      <td>-11.733570</td>\n",
       "      <td>1.206445</td>\n",
       "      <td>0.186851</td>\n",
       "      <td>-9.053557</td>\n",
       "      <td>-0.502555</td>\n",
       "      <td>0.059039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>80.00</td>\n",
       "      <td>98.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>2416.2400</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>25.918834</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232110</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.037359</td>\n",
       "      <td>-1.318564</td>\n",
       "      <td>0.337534</td>\n",
       "      <td>0.433320</td>\n",
       "      <td>-0.900990</td>\n",
       "      <td>0.147427</td>\n",
       "      <td>-0.895883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>61.00</td>\n",
       "      <td>97.700</td>\n",
       "      <td>62.500</td>\n",
       "      <td>3287.5000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>33.159902</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472733</td>\n",
       "      <td>0.673878</td>\n",
       "      <td>-1.013962</td>\n",
       "      <td>-1.412579</td>\n",
       "      <td>0.287313</td>\n",
       "      <td>0.025211</td>\n",
       "      <td>-2.841061</td>\n",
       "      <td>1.085241</td>\n",
       "      <td>-0.887060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    AGE  FEMALE  BP_SYSTOLIC  BP_DIASTOLIC  TEMPERATURE   PULSE     WEIGHT  \\\n",
       "0  84.0     0.0   197.000000         73.50       97.160  65.000  2576.1360   \n",
       "1  78.0     0.0   121.461538         68.00       98.275  83.000  2570.8500   \n",
       "2  75.0     0.0   108.750000         60.25       98.175  52.375  3275.5925   \n",
       "3  71.0     1.0   137.000000         80.00       98.000  71.000  2416.2400   \n",
       "4  71.0     1.0   109.500000         61.00       97.700  62.500  3287.5000   \n",
       "\n",
       "      HEIGHT        BMI   ALT  ...        27        28        29         30  \\\n",
       "0  67.514625  24.939445  22.0  ...  0.356847  1.847694  0.361279  -4.518900   \n",
       "1  75.545455  19.955741  22.5  ... -0.015880  3.052436 -0.607785 -10.509056   \n",
       "2  72.000000  27.762605  60.0  ...  0.021642  1.009353 -1.400726 -11.733570   \n",
       "3  64.000000  25.918834  22.0  ... -0.232110  0.114754  0.037359  -1.318564   \n",
       "4  66.000000  33.159902  22.0  ...  0.472733  0.673878 -1.013962  -1.412579   \n",
       "\n",
       "         31        32        33        34        35  DIABETES  \n",
       "0  0.127954  0.369101 -0.244062 -0.386845 -0.151202         1  \n",
       "1  0.301617  1.731270 -7.489346 -0.624842 -0.290411         0  \n",
       "2  1.206445  0.186851 -9.053557 -0.502555  0.059039         1  \n",
       "3  0.337534  0.433320 -0.900990  0.147427 -0.895883         0  \n",
       "4  0.287313  0.025211 -2.841061  1.085241 -0.887060         0  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = diabetes['DIABETES']\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((diabetes.values, treatments.values, diabetes.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((91,), (), (91,)), types: (tf.float64, tf.int64, tf.float64)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74282, 91)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def patient_generator(): \n",
    "    #iterator = dataset.make_one_shot_iterator()\n",
    "    sample = diabetes.sample(512)\n",
    "    treatment = sample.pop('DIABETES')\n",
    "#     y = sample['HGBA1C%']\n",
    "    y = sample['WEIGHT']\n",
    "\n",
    "    return (sample.values, treatment.values), y.values\n",
    "#### p_id, vitals, labs, sum of embeddings, age, female, whether or not diabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, t), y = patient_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 90)"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7cf10a966e90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatient_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatient_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1444fa0b7795>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-b89ea8f008d9>\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m                            \u001b[0mnonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"elu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                            \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mganite_weight_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                            beta=self.ganite_weight_beta)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f378209d74f5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, num_units, dropout, l2_weight, learning_rate, num_layers, num_treatments, with_bn, nonlinearity, initializer, alpha, beta)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_treatments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_treatments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "model.build(**kwargs)\n",
    "model.fit_generator(patient_generator, 10, patient_generator, 10, 500, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_predict_g_cf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-fc6e949a2521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_g_cf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_predict_g_cf'"
     ]
    }
   ],
   "source": [
    "(x_batch, t_batch), y_batch = patient_generator()\n",
    "\n",
    "t_batch = np.expand_dims(t_batch, axis=-1)\n",
    "y_batch = np.expand_dims(y_batch, axis=-1)\n",
    "\n",
    "\n",
    "y_pred = model.model._predict_g_cf([x_batch, t_batch], y_batch)\n",
    "\n",
    "print(y_pred[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5e42658b9c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "((y_pred[:, 1] - y_pred[:, 0]) > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2349.22      ],\n",
       "       [2483.26      ],\n",
       "       [2370.616     ],\n",
       "       [2915.365     ],\n",
       "       [1681.96333333],\n",
       "       [2737.23      ],\n",
       "       [2585.55      ],\n",
       "       [2505.92      ],\n",
       "       [2752.22      ],\n",
       "       [3435.06      ],\n",
       "       [2880.        ],\n",
       "       [2567.92      ],\n",
       "       [2987.675     ],\n",
       "       [3030.        ],\n",
       "       [3115.674     ],\n",
       "       [4866.8725    ],\n",
       "       [2331.832     ],\n",
       "       [2596.13666667],\n",
       "       [1803.11517241],\n",
       "       [2437.41      ],\n",
       "       [1944.984     ],\n",
       "       [2225.76      ],\n",
       "       [2926.53333333],\n",
       "       [2874.8       ],\n",
       "       [3143.955     ],\n",
       "       [2173.156     ],\n",
       "       [2931.23666667],\n",
       "       [2881.855     ],\n",
       "       [2804.762     ],\n",
       "       [2280.44      ],\n",
       "       [2522.06      ],\n",
       "       [3169.92      ],\n",
       "       [3200.        ],\n",
       "       [3664.42285714],\n",
       "       [3562.635     ],\n",
       "       [2807.77666667],\n",
       "       [1637.27428571],\n",
       "       [2231.055     ],\n",
       "       [2065.3925    ],\n",
       "       [4041.97784615],\n",
       "       [1600.        ],\n",
       "       [2720.        ],\n",
       "       [2239.87      ],\n",
       "       [2592.605     ],\n",
       "       [2186.965     ],\n",
       "       [2770.52      ],\n",
       "       [2885.70272727],\n",
       "       [3011.56666667],\n",
       "       [2773.78      ],\n",
       "       [3243.53142857],\n",
       "       [3114.65666667],\n",
       "       [2240.212     ],\n",
       "       [3058.22      ],\n",
       "       [5182.1425    ],\n",
       "       [2275.77666667],\n",
       "       [2448.27875   ],\n",
       "       [3416.992     ],\n",
       "       [1704.44      ],\n",
       "       [2060.6575    ],\n",
       "       [2560.86      ],\n",
       "       [2122.52307692],\n",
       "       [2209.13      ],\n",
       "       [3169.335     ],\n",
       "       [3075.06142857],\n",
       "       [1969.1475    ],\n",
       "       [2804.762     ],\n",
       "       [2515.06375   ],\n",
       "       [2373.91      ],\n",
       "       [2670.21333333],\n",
       "       [3310.72555556],\n",
       "       [3182.47625   ],\n",
       "       [3570.31333333],\n",
       "       [2698.43      ],\n",
       "       [2701.96      ],\n",
       "       [2892.44      ],\n",
       "       [3569.128     ],\n",
       "       [3534.41      ],\n",
       "       [6434.2475    ],\n",
       "       [2370.385     ],\n",
       "       [2350.39666667],\n",
       "       [3268.9775    ],\n",
       "       [4210.66142857],\n",
       "       [2018.96833333],\n",
       "       [2540.404     ],\n",
       "       [6373.64136364],\n",
       "       [2412.71      ],\n",
       "       [2639.49      ],\n",
       "       [2153.24619048],\n",
       "       [2618.47555556],\n",
       "       [2578.5       ],\n",
       "       [2939.13      ],\n",
       "       [3336.88      ],\n",
       "       [2857.401     ],\n",
       "       [3086.394     ],\n",
       "       [1756.82      ],\n",
       "       [3731.95      ],\n",
       "       [3308.66      ],\n",
       "       [3199.8225    ],\n",
       "       [2753.435     ],\n",
       "       [2781.325     ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
